import os
import shutil
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer, logging as hf_logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
sns.set_theme(style="whitegrid")

# –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ HuggingFace, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏
hf_logging.set_verbosity_info()

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
CACHE_DIR = "./llm_cache"
AVAILABLE_MODELS = {
    "1": {"name": "distilgpt2", "id": "distilgpt2", "desc": "–û—á–µ–Ω—å –ª–µ–≥–∫–∞—è (82M)"},
    "2": {"name": "gpt2", "id": "gpt2", "desc": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è (124M)"},
    "3": {"name": "DialoGPT-small", "id": "microsoft/DialoGPT-small", "desc": "–î–∏–∞–ª–æ–≥–æ–≤–∞—è (117M)"},
    "4": {"name": "TinyLlama-1.1B", "id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "desc": "–ú–æ—â–Ω–∞—è, –Ω–æ —Ç—è–∂–µ–ª–∞—è (1.1B)"},
}

class ModelManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ —É–¥–∞–ª–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π."""
    def __init__(self):
        self.current_model = None
        self.current_tokenizer = None
        self.model_name = None

        if not os.path.exists(CACHE_DIR):
            os.makedirs(CACHE_DIR)

    def is_downloaded(self, model_id):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –∫—ç—à–µ."""
        model_path = os.path.join(CACHE_DIR, f"models--{model_id.replace('/', '--')}")
        return os.path.exists(model_path)

    def load_model(self, selection_key):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å —Å –ª–æ–≥–∞–º–∏."""
        if selection_key not in AVAILABLE_MODELS:
            print("‚ùå –û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏.")
            return False

        model_info = AVAILABLE_MODELS[selection_key]
        print(f"\n" + "="*50)
        print(f"üöÄ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò: {model_info['name']}")
        print(f"üÜî ID –º–æ–¥–µ–ª–∏: {model_info['id']}")
        print("="*50)

        try:
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            print(f"\n[1/3] üìñ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            self.current_tokenizer = AutoTokenizer.from_pretrained(
                model_info['id'],
                cache_dir=CACHE_DIR
            )
            print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")

            # –§–∏–∫—Å –¥–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ pad_token
            if self.current_tokenizer.pad_token is None:
                self.current_tokenizer.pad_token = self.current_tokenizer.eos_token
                print("‚ÑπÔ∏è Pad token –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω EOS token.")

            # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            print(f"\n[2/3] üß† –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
            print("      (–ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è –≤–ø–µ—Ä–≤—ã–µ, –≤—ã —É–≤–∏–¥–∏—Ç–µ –ª–æ–≥–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –Ω–∏–∂–µ)\n")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}")

            self.current_model = AutoModelForCausalLM.from_pretrained(
                model_info['id'],
                cache_dir=CACHE_DIR,
                device_map="auto" if torch.cuda.is_available() else None, # auto –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å GPU
                dtype=torch.float16 if torch.cuda.is_available() else torch.float32 # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ torch_dtype -> dtype
            )

            # –ï—Å–ª–∏ device_map="auto" –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª (–Ω–∞ CPU –∏–Ω–æ–≥–¥–∞ –≥–ª—é—á–∏—Ç), –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Ä—É—á–Ω—É—é
            if device == "cpu":
                self.current_model.to("cpu")

            self.model_name = model_info['name']

            print(f"\n[3/3] ‚ú® –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è...")
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_name} –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
            print("="*50 + "\n")
            return True

        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ü–†–ò –ó–ê–ì–†–£–ó–ö–ï: {e}")
            import traceback
            traceback.print_exc()
            return False

    def delete_model(self, selection_key):
        """–£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞."""
        if selection_key not in AVAILABLE_MODELS:
            print("‚ùå –û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä.")
            return

        model_id = AVAILABLE_MODELS[selection_key]['id']
        folder_name = f"models--{model_id.replace('/', '--')}"
        path = os.path.join(CACHE_DIR, folder_name)

        if os.path.exists(path):
            try:
                shutil.rmtree(path)
                print(f"üóëÔ∏è –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ {AVAILABLE_MODELS[selection_key]['name']} —É–¥–∞–ª–µ–Ω—ã.")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è: {e}")
        else:
            print(f"‚ö†Ô∏è –§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

    def unload_model(self):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏."""
        if self.current_model:
            print("üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
            del self.current_model
            del self.current_tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.current_model = None
            self.current_tokenizer = None
            self.model_name = None
            print("‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞.")
        else:
            print("‚ö†Ô∏è –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.")

class ExperimentRunner:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π."""
    def __init__(self, manager):
        self.manager = manager

    def generate_text(self, prompt, **kwargs):
        if not self.manager.current_model:
            print("‚ùå –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å!")
            return None

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        inputs = self.manager.current_tokenizer(prompt, return_tensors="pt")
        inputs = inputs.to(self.manager.current_model.device)

        gen_kwargs = {
            "max_new_tokens": 50,
            "do_sample": True,
            "pad_token_id": self.manager.current_tokenizer.pad_token_id
        }
        gen_kwargs.update(kwargs)

        try:
            # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –º–æ–º–µ–Ω—Ç —Å–∞–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–µ –º—É—Å–æ—Ä–∏—Ç—å
            hf_logging.set_verbosity_error()

            with torch.no_grad():
                outputs = self.manager.current_model.generate(**inputs, **gen_kwargs)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
            hf_logging.set_verbosity_info()

            text = self.manager.current_tokenizer.decode(outputs[0], skip_special_tokens=True)
            return text
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None

    def run_temperature_experiment(self):
        print("\n--- üå°Ô∏è –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: TEMPERATURE ---")
        prompt = "The future of artificial intelligence is"
        temps = [0.1, 0.4, 0.7, 1.0]
        results = []

        print(f"–ü—Ä–æ–º–ø—Ç: '{prompt}'\n")

        for t in temps:
            print(f"‚öôÔ∏è Temperature = {t}...")
            text = self.generate_text(prompt, temperature=t, top_k=50)
            print(f"‚û§ –†–µ–∑—É–ª—å—Ç–∞—Ç: {text[len(prompt):].strip()}...\n")
            results.append(len(text))

        plt.figure(figsize=(8, 4))
        plt.plot(temps, results, marker='o')
        plt.title(f"–î–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ vs Temperature ({self.manager.model_name})")
        plt.xlabel("Temperature")
        plt.ylabel("–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª—ã)")
        plt.savefig("temp_experiment.png")
        print("üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ 'temp_experiment.png'")

    def run_sampling_experiment(self):
        print("\n--- üé≤ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: TOP-K –∏ TOP-P ---")
        prompt = "Once upon a time in a galaxy far away"

        configs = [
            {"top_k": 5, "top_p": None, "name": "Top-k=5 (Strict)"},
            {"top_k": 50, "top_p": None, "name": "Top-k=50 (Diverse)"},
            {"top_k": None, "top_p": 0.3, "name": "Top-p=0.3 (Nucleus Strict)"},
            {"top_k": None, "top_p": 0.9, "name": "Top-p=0.9 (Nucleus Creative)"},
        ]

        combinations = [
            {"top_k": 50, "top_p": 0.9},
            {"top_k": 10, "top_p": 0.5},
            {"top_k": 100, "top_p": 0.95}
        ]

        print(f"–ü—Ä–æ–º–ø—Ç: '{prompt}'\n")

        for conf in configs:
            kwargs = {k: v for k, v in conf.items() if k != "name" and v is not None}
            print(f"‚öôÔ∏è {conf['name']}...")
            text = self.generate_text(prompt, **kwargs)
            if text:
                print(f"‚û§ {text[len(prompt):].strip()}...\n")

        print("--- –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏ ---")
        for comb in combinations:
            print(f"‚öôÔ∏è k={comb['top_k']}, p={comb['top_p']}...")
            text = self.generate_text(prompt, top_k=comb['top_k'], top_p=comb['top_p'])
            if text:
                print(f"‚û§ {text[len(prompt):].strip()}...\n")

    def run_prompt_types(self):
        print("\n--- üìù –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: –¢–ò–ü–´ –ü–†–û–ú–ü–¢–û–í ---")
        prompts = {
            "–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ": "Python is the best language because",
            "–í–æ–ø—Ä–æ—Å": "What is the capital of France?",
            "–¢–≤–æ—Ä—á–µ—Å—Ç–≤–æ": "Write a short poem about a robot:",
            "–°–ø–∏—Å–æ–∫": "List 5 fruits:\n1.",
        }

        for p_type, prompt in prompts.items():
            print(f"\nüîπ –¢–∏–ø: {p_type} | –ü—Ä–æ–º–ø—Ç: {prompt}")
            text = self.generate_text(prompt, temperature=0.7, top_k=40)
            if text:
                print(f"‚û§ –û—Ç–≤–µ—Ç: {text}")

def main_menu():
    manager = ModelManager()
    runner = ExperimentRunner(manager)

    while True:
        print("\n" + "="*40)
        print(f"ü§ñ LLM LAB CONTROL PANEL | –ú–æ–¥–µ–ª—å: {manager.model_name if manager.model_name else '–ù–µ –≤—ã–±—Ä–∞–Ω–∞'}")
        print("="*40)
        print("1. üìÇ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏")
        print("2. üå°Ô∏è –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Temperature")
        print("3. üé≤ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Top-k / Top-p")
        print("4. üìù –¢–µ—Å—Ç —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤")
        print("5. ‚úçÔ∏è –†—É—á–Ω–æ–π –≤–≤–æ–¥")
        print("0. üö™ –í—ã—Ö–æ–¥")

        choice = input("\n–í–∞—à –≤—ã–±–æ—Ä: ")

        if choice == "1":
            while True:
                print("\n--- –ú–ï–ù–Æ –ú–û–î–ï–õ–ï–ô ---")
                for k, v in AVAILABLE_MODELS.items():
                    status = "üíæ (–°–∫–∞—á–∞–Ω–∞)" if manager.is_downloaded(v['id']) else "‚òÅÔ∏è (–ù—É–∂–Ω–æ –∫–∞—á–∞—Ç—å)"
                    current = "‚≠ê [–ê–ö–¢–ò–í–ù–ê]" if manager.model_name == v['name'] else ""
                    print(f"{k}. {v['name']} | {status} {current}")

                print("U. Unload (–û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å)")
                print("D. Delete (–£–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª—ã)")
                print("B. Back (–ù–∞–∑–∞–¥)")

                sub = input("–í—ã–±–æ—Ä: ").lower()
                if sub == 'b': break
                elif sub == 'u': manager.unload_model()
                elif sub == 'd':
                    key = input("–ù–æ–º–µ—Ä –º–æ–¥–µ–ª–∏: ")
                    manager.delete_model(key)
                elif sub in AVAILABLE_MODELS:
                    manager.load_model(sub)
                else: print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥.")

        elif choice == "2":
            if manager.current_model: runner.run_temperature_experiment()
            else: print("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (–ü—É–Ω–∫—Ç 1)")
        elif choice == "3":
            if manager.current_model: runner.run_sampling_experiment()
            else: print("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (–ü—É–Ω–∫—Ç 1)")
        elif choice == "4":
            if manager.current_model: runner.run_prompt_types()
            else: print("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (–ü—É–Ω–∫—Ç 1)")
        elif choice == "5":
            if manager.current_model:
                p = input("–ü—Ä–æ–º–ø—Ç: ")
                res = runner.generate_text(p, temperature=0.7)
                if res: print(f"\n‚û§ {res}")
            else: print("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (–ü—É–Ω–∫—Ç 1)")
        elif choice == "0": break
        else: print("–û—à–∏–±–∫–∞ –≤–≤–æ–¥–∞")

if __name__ == "__main__":
    try:
        main_menu()
    except KeyboardInterrupt:
        print("\n–í—ã—Ö–æ–¥.")